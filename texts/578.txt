The grammatical analysis of texts in any written language typically involves
a number of basic processing tasks, such as tokenization, morphological
tagging, and dependency parsing. State-of-the-art systems can achieve high
accuracy on these tasks for languages with large datasets, but yield poor
results for languages which have little to no annotated data. To address this
issue for the Tagalog language, we investigate the use of alternative language
resources for creating task-specific models in the absence of
dependency-annotated Tagalog data. We also explore the use of word embeddings
and data augmentation to improve performance when only a small amount of
annotated Tagalog data is available. We show that these zero-shot and few-shot
approaches yield substantial improvements on grammatical analysis of both
in-domain and out-of-domain Tagalog text compared to state-of-the-art
supervised baselines.