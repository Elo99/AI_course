Recently, Graph Neural Networks (GNNs) achieve remarkable success in
Recommendation. To reduce the influence of data sparsity, Graph Contrastive
Learning (GCL) is adopted in GNN-based CF methods for enhancing performance.
Most GCL methods consist of data augmentation and contrastive loss (e.g.,
InfoNCE). GCL methods construct the contrastive pairs by hand-crafted graph
augmentations and maximize the agreement between different views of the same
node compared to that of other nodes, which is known as the InfoMax principle.
However, improper data augmentation will hinder the performance of GCL. InfoMin
principle, that the good set of views shares minimal information and gives
guidelines to design better data augmentation. In this paper, we first propose
a new data augmentation (i.e., edge-operating including edge-adding and
edge-dropping). Then, guided by InfoMin principle, we propose a novel
theoretical guiding contrastive learning framework, named Learnable Data
Augmentation for Graph Contrastive Learning (LDA-GCL). Our methods include data
augmentation learning and graph contrastive learning, which follow the InfoMin
and InfoMax principles, respectively. In implementation, our methods optimize
the adversarial loss function to learn data augmentation and effective
representations of users and items. Extensive experiments on four public
benchmark datasets demonstrate the effectiveness of LDA-GCL.