We study the problem of recognizing visual entities from the textual
descriptions of their classes. Specifically, given birds' images with free-text
descriptions of their species, we learn to classify images of previously-unseen
species based on specie descriptions. This setup has been studied in the vision
community under the name zero-shot learning from text, focusing on learning to
transfer knowledge about visual aspects of birds from seen classes to
previously-unseen ones. Here, we suggest focusing on the textual description
and distilling from the description the most relevant information to
effectively match visual features to the parts of the text that discuss them.
Specifically, (1) we propose to leverage the similarity between species,
reflected in the similarity between text descriptions of the species. (2) we
derive visual summaries of the texts, i.e., extractive summaries that focus on
the visual features that tend to be reflected in images. We propose a simple
attention-based model augmented with the similarity and visual summaries
components. Our empirical results consistently and significantly outperform the
state-of-the-art on the largest benchmarks for text-based zero-shot learning,
illustrating the critical importance of texts for zero-shot image-recognition.