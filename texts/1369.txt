This paper describes our NPU-ASLP system submitted to the ISCSLP 2022
Magichub Code-Switching ASR Challenge. In this challenge, we first explore
several popular end-to-end ASR architectures and training strategies, including
bi-encoder, language-aware encoder (LAE) and mixture of experts (MoE). To
improve our system's language modeling ability, we further attempt the internal
language model as well as the long context language model. Given the limited
training data in the challenge, we further investigate the effects of data
augmentation, including speed perturbation, pitch shifting, speech codec,
SpecAugment and synthetic data from text-to-speech (TTS). Finally, we explore
ROVER-based score fusion to make full use of complementary hypotheses from
different models. Our submitted system achieves 16.87% on mix error rate (MER)
on the test set and comes to the 2nd place in the challenge ranking.