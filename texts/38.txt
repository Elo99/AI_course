This paper explores the possibilities of image style transfer applied to text
maintaining the original transcriptions. Results on different text domains
(scene text, machine printed text and handwritten text) and cross modal results
demonstrate that this is feasible, and open different research lines.
Furthermore, two architectures for selective style transfer, which means
transferring style to only desired image pixels, are proposed. Finally, scene
text selective style transfer is evaluated as a data augmentation technique to
expand scene text detection datasets, resulting in a boost of text detectors
performance. Our implementation of the described models is publicly available.