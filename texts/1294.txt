Emotion Recognition in Conversations (ERC) is an important and active
research area. Recent work has shown the benefits of using multiple modalities
(e.g., text, audio, and video) for the ERC task. In a conversation,
participants tend to maintain a particular emotional state unless some stimuli
evokes a change. There is a continuous ebb and flow of emotions in a
conversation. Inspired by this observation, we propose a multimodal ERC model
and augment it with an emotion-shift component that improves performance. The
proposed emotion-shift component is modular and can be added to any existing
multimodal ERC model (with a few modifications). We experiment with different
variants of the model, and results show that the inclusion of emotion shift
signal helps the model to outperform existing models for ERC on MOSEI and
IEMOCAP datasets.