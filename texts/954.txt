Data augmentation is practically helpful for visual recognition, especially
at the time of data scarcity. However, such success is only limited to quite a
few light augmentations (e.g., random crop, flip). Heavy augmentations (e.g.,
gray, grid shuffle) are either unstable or show adverse effects during
training, owing to the big gap between the original and augmented images. This
paper introduces a novel network design, noted as Augmentation Pathways (AP),
to systematically stabilize training on a much wider range of augmentation
policies. Notably, AP tames heavy data augmentations and stably boosts
performance without a careful selection among augmentation policies. Unlike
traditional single pathway, augmented images are processed in different neural
paths. The main pathway handles light augmentations, while other pathways focus
on heavy augmentations. By interacting with multiple paths in a dependent
manner, the backbone network robustly learns from shared visual patterns among
augmentations, and suppresses noisy patterns at the same time. Furthermore, we
extend AP to a homogeneous version and a heterogeneous version for high-order
scenarios, demonstrating its robustness and flexibility in practical usage.
Experimental results on ImageNet benchmarks demonstrate the compatibility and
effectiveness on a much wider range of augmentations (e.g., Crop, Gray, Grid
Shuffle, RandAugment), while consuming fewer parameters and lower computational
costs at inference time. Source code:https://github.com/ap-conv/ap-net.