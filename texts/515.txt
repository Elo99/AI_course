Synthesis planning and reaction outcome prediction are two fundamental
problems in computer-aided organic chemistry for which a variety of data-driven
approaches have emerged. Natural language approaches that model each problem as
a SMILES-to-SMILES translation lead to a simple end-to-end formulation, reduce
the need for data preprocessing, and enable the use of well-optimized machine
translation model architectures. However, SMILES representations are not an
efficient representation for capturing information about molecular structures,
as evidenced by the success of SMILES augmentation to boost empirical
performance. Here, we describe a novel Graph2SMILES model that combines the
power of Transformer models for text generation with the permutation invariance
of molecular graph encoders that mitigates the need for input data
augmentation. As an end-to-end architecture, Graph2SMILES can be used as a
drop-in replacement for the Transformer in any task involving
molecule(s)-to-molecule(s) transformations. In our encoder, an
attention-augmented directed message passing neural network (D-MPNN) captures
local chemical environments, and the global attention encoder allows for
long-range and intermolecular interactions, enhanced by graph-aware positional
embedding. Graph2SMILES improves the top-1 accuracy of the Transformer
baselines by $1.7\%$ and $1.9\%$ for reaction outcome prediction on USPTO_480k
and USPTO_STEREO datasets respectively, and by $9.8\%$ for one-step
retrosynthesis on the USPTO_50k dataset.