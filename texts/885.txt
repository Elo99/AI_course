Motivated by variational models in continuum mechanics, we introduce a novel
algorithm to perform nonsmooth and nonconvex minimizations with linear
constraints in Euclidean spaces. We show how this algorithm is actually a
natural generalization of the well-known non-stationary augmented Lagrangian
method for convex optimization. The relevant features of this approach are its
applicability to a large variety of nonsmooth and nonconvex objective
functions, its guaranteed convergence to critical points of the objective
energy independently of the choice of the initial value, and its simplicity of
implementation. In fact, the algorithm results in a nested double loop
iteration. In the inner loop an augmented Lagrangian algorithm performs an
adaptive finite number of iterations on a fixed quadratic and strictly convex
perturbation of the objective energy, depending on a parameter which is adapted
by the external loop. To show the versatility of this new algorithm, we
exemplify how it can be used for computing critical points in inverse
free-discontinuity variational models, such as the Mumford-Shah functional,
and, by doing so, we also derive and analyze new iterative thresholding
algorithms.