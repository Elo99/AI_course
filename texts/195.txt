In this paper, we propose MixSpeech, a simple yet effective data augmentation
method based on mixup for automatic speech recognition (ASR). MixSpeech trains
an ASR model by taking a weighted combination of two different speech features
(e.g., mel-spectrograms or MFCC) as the input, and recognizing both text
sequences, where the two recognition losses use the same combination weight. We
apply MixSpeech on two popular end-to-end speech recognition models including
LAS (Listen, Attend and Spell) and Transformer, and conduct experiments on
several low-resource datasets including TIMIT, WSJ, and HKUST. Experimental
results show that MixSpeech achieves better accuracy than the baseline models
without data augmentation, and outperforms a strong data augmentation method
SpecAugment on these recognition tasks. Specifically, MixSpeech outperforms
SpecAugment with a relative PER improvement of 10.6$\%$ on TIMIT dataset, and
achieves a strong WER of 4.7$\%$ on WSJ dataset.