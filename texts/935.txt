The high computational complexity and energy consumption of artificial
intelligence (AI) algorithms hinder their application in augmented reality (AR)
systems. However, mobile edge computing (MEC) makes it possible to solve this
problem. This paper considers the scene of completing video-based AI inference
tasks in the MEC system. We formulate a mixed-integer nonlinear programming
problem (MINLP) to reduce inference delays, energy consumption and to improve
recognition accuracy. We give a simplified expression of the inference
complexity model and accuracy model through derivation and experimentation. The
problem is then solved iteratively by using alternating optimization.
Specifically, by assuming that the offloading decision is given, the problem is
decoupled into two sub-problems, i.e., the resource allocation problem for the
devices set that completes the inference tasks locally, and that for the
devices set that offloads tasks. For the problem of offloading decision
optimization, we propose a Channel-Aware heuristic algorithm. To further reduce
the complexity, we propose an alternating direction method of multipliers
(ADMM) based distributed algorithm. The ADMM-based algorithm has a low
computational complexity that grows linearly with the number of devices.
Numerical experiments show the effectiveness of proposed algorithms. The
trade-off relationship between delay, energy consumption, and accuracy is also
analyzed.