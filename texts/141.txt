GPT-3 is a large-scale natural language model developed by OpenAI that can
perform many different tasks, including topic classification. Although
researchers claim that it requires only a small number of in-context examples
to learn a task, in practice GPT-3 requires these training examples to be
either of exceptional quality or a higher quantity than easily created by hand.
To address this issue, this study teaches GPT-3 to classify whether a question
is related to data science by augmenting a small training set with additional
examples generated by GPT-3 itself. This study compares two classifiers: the
GPT-3 Classification Endpoint with augmented examples, and the GPT-3 Completion
Endpoint with an optimal training set chosen using a genetic algorithm. We find
that while the augmented Completion Endpoint achieves upwards of 80 percent
validation accuracy, using the augmented Classification Endpoint yields more
consistent accuracy on unseen examples. In this way, giving large-scale machine
learning models like GPT-3 the ability to propose their own additional training
examples can result in improved classification performance.