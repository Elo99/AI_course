Speech-to-text errors made by automatic speech recognition (ASR) system
negatively impact downstream models relying on ASR transcriptions. Language
error correction models as a post-processing text editing approach have been
recently developed for refining the source sentences. However, efficient models
for correcting errors in ASR transcriptions that meet the low latency
requirements of industrial grade production systems have not been well studied.
In this work, we propose a novel non-autoregressive (NAR) error correction
approach to improve the transcription quality by reducing word error rate (WER)
and achieve robust performance across different upstream ASR systems. Our
approach augments the text encoding of the Transformer model with a phoneme
encoder that embeds pronunciation information. The representations from phoneme
encoder and text encoder are combined via multi-modal fusion before feeding
into the length tagging predictor for predicting target sequence lengths. The
joint encoders also provide inputs to the attention mechanism in the NAR
decoder. We experiment on 3 open-source ASR systems with varying speech-to-text
transcription quality and their erroneous transcriptions on 2 public English
corpus datasets. Results show that our PATCorrect (Phoneme Augmented
Transformer for ASR error Correction) consistently outperforms state-of-the-art
NAR error correction method on English corpus across different upstream ASR
systems. For example, PATCorrect achieves 11.62% WER reduction (WERR) averaged
on 3 ASR systems compared to 9.46% WERR achieved by other method using text
only modality and also achieves an inference latency comparable to other NAR
models at tens of millisecond scale, especially on GPU hardware, while still
being 4.2 - 6.7x times faster than autoregressive models on Common Voice and
LibriSpeech datasets.