Recent advances in data augmentation enable one to translate images by
learning the mapping between a source domain and a target domain. Existing
methods tend to learn the distributions by training a model on a variety of
datasets, with results evaluated largely in a subjective manner. Relatively few
works in this area, however, study the potential use of image synthesis methods
for recognition tasks. In this paper, we propose and explore the problem of
image translation for data augmentation. We first propose a lightweight yet
efficient model for translating texture to augment images based on a single
input of source texture, allowing for fast training and testing, referred to as
Single Image Texture Translation for data Augmentation (SITTA). Then we explore
the use of augmented data in long-tailed and few-shot image classification
tasks. We find the proposed augmentation method and workflow is capable of
translating the texture of input data into a target domain, leading to
consistently improved image recognition performance. Finally, we examine how
SITTA and related image translation methods can provide a basis for a
data-efficient, "augmentation engineering" approach to model training. Codes
are available at https://github.com/Boyiliee/SITTA.