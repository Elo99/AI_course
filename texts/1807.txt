Data augmentation has proved extremely useful by increasing training data
variance to alleviate overfitting and improve deep neural networks'
generalization performance. In medical image analysis, a well-designed
augmentation policy usually requires much expert knowledge and is difficult to
generalize to multiple tasks due to the vast discrepancies among pixel
intensities, image appearances, and object shapes in different medical tasks.
To automate medical data augmentation, we propose a regularized adversarial
training framework via two min-max objectives and three differentiable
augmentation models covering affine transformation, deformation, and appearance
changes. Our method is more automatic and efficient than previous automatic
augmentation methods, which still rely on pre-defined operations with
human-specified ranges and costly bi-level optimization. Extensive experiments
demonstrated that our approach, with less training overhead, achieves superior
performance over state-of-the-art auto-augmentation methods on both tasks of 2D
skin cancer classification and 3D organs-at-risk segmentation.