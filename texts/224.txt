Despite the success of mixup in data augmentation, its applicability to
natural language processing (NLP) tasks has been limited due to the discrete
and variable-length nature of natural languages. Recent studies have thus
relied on domain-specific heuristics and manually crafted resources, such as
dictionaries, in order to apply mixup in NLP. In this paper, we instead propose
an unsupervised learning approach to text interpolation for the purpose of data
augmentation, to which we refer as "Learning to INterpolate for Data
Augmentation" (LINDA), that does not require any heuristics nor manually
crafted resources but learns to interpolate between any pair of natural
language sentences over a natural language manifold. After empirically
demonstrating the LINDA's interpolation capability, we show that LINDA indeed
allows us to seamlessly apply mixup in NLP and leads to better generalization
in text classification both in-domain and out-of-domain.