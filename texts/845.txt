Free-text rationales (FTRs) follow how humans communicate by explaining
reasoning processes via natural language. A number of recent works have studied
how to improve language model (LM) generalization by using FTRs to teach LMs
the correct reasoning processes behind correct task outputs. These prior works
aim to learn from FTRs by appending them to the LM input or target output, but
this may introduce an input distribution shift or conflict with the task
objective, respectively. We propose KNIFE, which distills FTR knowledge from an
FTR-augmented teacher LM (takes both task input and FTR) to a student LM (takes
only task input), which is used for inference. Crucially, the teacher LM's
forward computation has a bottleneck stage in which all of its FTR states are
masked out, which pushes knowledge from the FTR states into the task
input/output states. Then, FTR knowledge is distilled to the student LM by
training its task input/output states to align with the teacher LM's. On two
question answering datasets, we show that KNIFE significantly outperforms
existing FTR learning methods, in both fully-supervised and low-resource
settings.