Text data augmentation is an effective strategy for overcoming the challenge
of limited sample sizes in many natural language processing (NLP) tasks. This
challenge is especially prominent in the few-shot learning scenario, where the
data in the target domain is generally much scarcer and of lowered quality. A
natural and widely-used strategy to mitigate such challenges is to perform data
augmentation on the training data to better capture the data invariance and
increase the sample size. However, current text data augmentation methods
either can not ensure the correct labeling of the generated data (lacking
faithfulness) or can not ensure sufficient diversity in the generated data
(lacking completeness), or both. Inspired by the recent success of large
language models, especially the development of ChatGPT, which demonstrated
improved language comprehension abilities, in this work, we propose a text data
augmentation approach based on ChatGPT (named ChatAug). ChatGPT is trained on
data with unparalleled linguistic richness and employs a reinforcement training
process with large-scale human feedback, which endows the model with affinity
to the naturalness of human language. Our text data augmentation approach
ChatAug rephrases each sentence in the training samples into multiple
conceptually similar but semantically different samples. The augmented samples
can then be used in downstream model training. Experiment results on few-shot
learning text classification tasks show the superior performance of the
proposed ChatAug approach over state-of-the-art text data augmentation methods
in terms of testing accuracy and distribution of the augmented samples.