Using machine learning in high-stakes applications often requires predictions
to be accompanied by explanations comprehensible to the domain user, who has
ultimate responsibility for decisions and outcomes. Recently, a new framework
for providing explanations, called TED, has been proposed to provide meaningful
explanations for predictions. This framework augments training data to include
explanations elicited from domain users, in addition to features and labels.
This approach ensures that explanations for predictions are tailored to the
complexity expectations and domain knowledge of the consumer. In this paper, we
build on this foundational work, by exploring more sophisticated instantiations
of the TED framework and empirically evaluate their effectiveness in two
diverse domains, chemical odor and skin cancer prediction. Results demonstrate
that meaningful explanations can be reliably taught to machine learning
algorithms, and in some cases, improving modeling accuracy.